{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf80682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 60 markdown files.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# base path\n",
    "file_path = Path(\"papers 250\") / \"60\"\n",
    "\n",
    "md_contents = []\n",
    "\n",
    "for folder in file_path.iterdir():\n",
    "    if folder.is_dir():\n",
    "        doc_folder = folder / \"document\"\n",
    "        if doc_folder.exists():\n",
    "            for md_file in doc_folder.glob(\"*.checked.md\"):\n",
    "                with md_file.open(encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                md_contents.append(content)\n",
    "\n",
    "# Now md_contents is a list of strings\n",
    "print(f\"Collected {len(md_contents)} markdown files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7fc1a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp38-cp38-macosx_10_9_x86_64.whl (961 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.0/962.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp38-cp38-macosx_10_9_x86_64.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /Users/lindazhou/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/lindazhou/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lindazhou/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lindazhou/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lindazhou/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2022.9.24)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.11.6 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Excel with hidden columns to token_costs_gpt5_claude.xlsx\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Pricing (USD per 1M tokens → per 1000 tokens)\n",
    "pricing = {\n",
    "    # GPT Models\n",
    "    \"GPT-5\": {\n",
    "        \"input\": 1.25,\n",
    "        \"output\": 10.00,\n",
    "        \"cached\": 0.125\n",
    "    },\n",
    "    \"GPT-5 mini\": {\n",
    "        \"input\": 0.25,\n",
    "        \"output\": 2.00,\n",
    "        \"cached\": 0.025\n",
    "    },\n",
    "    \"GPT-5 nano\": {\n",
    "        \"input\": 0.05,\n",
    "        \"output\": 0.40,\n",
    "        \"cached\": 0.005\n",
    "    },\n",
    "    \"GPT-5 Chat Latest\": {\n",
    "        \"input\": 1.25,\n",
    "        \"output\": 10.00,\n",
    "        \"cached\": 0.125\n",
    "    },\n",
    "    \"GPT-5 Codex\": {\n",
    "        \"input\": 1.25,\n",
    "        \"output\": 10.00,\n",
    "        \"cached\": 0.125\n",
    "    },\n",
    "    \"GPT-4.1\": {\n",
    "        \"input\": 2.00,\n",
    "        \"output\": 8.00,\n",
    "        \"cached\": 0.50\n",
    "    },\n",
    "    \"GPT-4.1 mini\": {\n",
    "        \"input\": 0.40,\n",
    "        \"output\": 1.60,\n",
    "        \"cached\": 0.10\n",
    "    },\n",
    "    \"GPT-4.1 nano\": {\n",
    "        \"input\": 0.10,\n",
    "        \"output\": 0.40,\n",
    "        \"cached\": 0.025\n",
    "    },\n",
    "    \"GPT-4o\": {\n",
    "        \"input\": 2.50,\n",
    "        \"output\": 10.00,\n",
    "        \"cached\": 1.25\n",
    "    },\n",
    "\n",
    "    # DeepSeek Models\n",
    "    \"DeepSeek Chat V3.1 (Non-thinking)\": {\n",
    "        \"input\": 0.56,\n",
    "        \"output\": 1.68,\n",
    "        \"cached\": 0.07\n",
    "    },\n",
    "    \"DeepSeek Reasoner V3.1 (Thinking)\": {\n",
    "        \"input\": 0.56,\n",
    "        \"output\": 1.68,\n",
    "        \"cached\": 0.07\n",
    "    },\n",
    "\n",
    "    # Claude Models\n",
    "    \"Claude Opus 4.1\": {\n",
    "        \"input\": 15,\n",
    "        \"output\": 75,\n",
    "        \"cached_5m\": 18.75,\n",
    "        \"cached_1h\": 30,\n",
    "        \"cache_hit\": 1.50\n",
    "    },\n",
    "    \"Claude Opus 4\": {\n",
    "        \"input\": 15,\n",
    "        \"output\": 75,\n",
    "        \"cached_5m\": 18.75,\n",
    "        \"cached_1h\": 30,\n",
    "        \"cache_hit\": 1.50\n",
    "    },\n",
    "    \"Claude Sonnet 4\": {\n",
    "        \"input\": 3,\n",
    "        \"output\": 15,\n",
    "        \"cached_5m\": 3.75,\n",
    "        \"cached_1h\": 6,\n",
    "        \"cache_hit\": 0.30\n",
    "    },\n",
    "    \"Claude Sonnet 3.7\": {\n",
    "        \"input\": 3,\n",
    "        \"output\": 15,\n",
    "        \"cached_5m\": 3.75,\n",
    "        \"cached_1h\": 6,\n",
    "        \"cache_hit\": 0.30\n",
    "    },\n",
    "    \"Claude Haiku 3.5\": {\n",
    "        \"input\": 0.80,\n",
    "        \"output\": 4,\n",
    "        \"cached_5m\": 1,\n",
    "        \"cached_1h\": 1.6,\n",
    "        \"cache_hit\": 0.08\n",
    "    },\n",
    "    \"Claude Haiku 3\": {\n",
    "        \"input\": 0.25,\n",
    "        \"output\": 1.25,\n",
    "        \"cached_5m\": 0.30,\n",
    "        \"cached_1h\": 0.50,\n",
    "        \"cache_hit\": 0.03\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Fixed output tokens\n",
    "fixed_output_tokens = 15000\n",
    "\n",
    "# Map each model to its tiktoken encoding\n",
    "model_encodings = {\n",
    "    \"GPT-5\": \"gpt-4\",\n",
    "    \"GPT-5 mini\": \"gpt-4\",\n",
    "    \"GPT-5 nano\": \"gpt-4\",\n",
    "    \"GPT-5 Chat Latest\": \"gpt-4\",\n",
    "    \"GPT-5 Codex\": \"gpt-4\",\n",
    "    \"GPT-4.1\": \"gpt-4\",\n",
    "    \"GPT-4.1 mini\": \"gpt-4\",\n",
    "    \"GPT-4.1 nano\": \"gpt-4\",\n",
    "    \"GPT-4o\": \"gpt-4\",\n",
    "    \"DeepSeek Chat V3.1 (Non-thinking)\": \"gpt-4\",\n",
    "    \"DeepSeek Reasoner V3.1 (Thinking)\": \"gpt-4\",\n",
    "    \"Claude Opus 4.1\": \"cl100k_base\",\n",
    "    \"Claude Opus 4\": \"cl100k_base\",\n",
    "    \"Claude Sonnet 4\": \"cl100k_base\",\n",
    "    \"Claude Sonnet 3.7\": \"cl100k_base\",\n",
    "    \"Claude Haiku 3.5\": \"cl100k_base\",\n",
    "    \"Claude Haiku 3\": \"cl100k_base\",\n",
    "}\n",
    "\n",
    "# # Tokenizer (closest available to GPT-5 & Claude)\n",
    "# encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "file_path = Path(\"papers 250\") / \"60\"\n",
    "rows = []\n",
    "\n",
    "for folder in file_path.iterdir():\n",
    "    if folder.is_dir():\n",
    "        doc_folder = folder / \"document\"\n",
    "        if doc_folder.exists():\n",
    "            for md_file in doc_folder.glob(\"*.checked.md\"):\n",
    "                with md_file.open(encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                table_name = folder.name\n",
    "                row = {\"Table Name\": table_name}\n",
    "\n",
    "                # Compute base tokens for the document (initial input token count)\n",
    "                base_enc = tiktoken.encoding_for_model(\"gpt-4\")  # or any standard model\n",
    "                base_tokens = len(base_enc.encode(content))\n",
    "                row[\"Tokens\"] = base_tokens  # store it for reference / totals\n",
    "\n",
    "                # Now loop over models to calculate costs\n",
    "                for model, rates in pricing.items():\n",
    "                    encoding_name = model_encodings.get(model, \"gpt-4\")\n",
    "                    \n",
    "                    # Use correct encoding per model\n",
    "                    if encoding_name.startswith(\"cl\"):\n",
    "                        enc = tiktoken.get_encoding(encoding_name)\n",
    "                    else:\n",
    "                        enc = tiktoken.encoding_for_model(encoding_name)\n",
    "                    \n",
    "                    model_tokens = len(enc.encode(content))  # optional per-model token count\n",
    "\n",
    "                    # GPT-5 style models\n",
    "                    if \"cached\" in rates:\n",
    "                        input_cost = model_tokens * rates[\"input\"] / 1_000_000\n",
    "                        output_cost = fixed_output_tokens * rates[\"output\"] / 1_000_000\n",
    "                        cached_cost = model_tokens * rates[\"cached\"] / 1_000_000\n",
    "                        total_cost = input_cost + output_cost\n",
    "                        row[f\"{model} (input)\"] = input_cost\n",
    "                        row[f\"{model} (output, 4000)\"] = output_cost\n",
    "                        row[f\"{model} (cached)\"] = cached_cost\n",
    "                        row[f\"{model} (total)\"] = total_cost\n",
    "\n",
    "                    # Claude style models\n",
    "                    else:\n",
    "                        input_cost = model_tokens * rates[\"input\"] / 1_000\n",
    "                        output_cost = fixed_output_tokens * rates[\"output\"] / 1_000\n",
    "                        cache_5m = model_tokens * rates[\"cached_5m\"] / 1_000\n",
    "                        cache_1h = model_tokens * rates[\"cached_1h\"] / 1_000\n",
    "                        cache_hit = model_tokens * rates[\"cache_hit\"] / 1_000\n",
    "                        total_cost = input_cost + output_cost\n",
    "                        row[f\"{model} (input)\"] = input_cost\n",
    "                        row[f\"{model} (output, 4000)\"] = output_cost\n",
    "                        row[f\"{model} (cache_5m)\"] = cache_5m\n",
    "                        row[f\"{model} (cache_1h)\"] = cache_1h\n",
    "                        row[f\"{model} (cache_hit)\"] = cache_hit\n",
    "                        row[f\"{model} (total)\"] = total_cost\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df_filled = df.fillna(0)\n",
    "\n",
    "# Create totals row\n",
    "totals = {\"Table Name\": \"TOTAL\"}\n",
    "for col in df_filled.columns:\n",
    "    if col != \"Table Name\":  # sum all numeric columns\n",
    "        totals[col] = df_filled[col].sum()\n",
    "\n",
    "# Append totals row\n",
    "df = pd.concat([df_filled, pd.DataFrame([totals])], ignore_index=True)\n",
    "\n",
    "\n",
    "# Save to Excel\n",
    "output_file = \"token_costs_gpt5_claude.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Load workbook with openpyxl\n",
    "wb = load_workbook(output_file)\n",
    "ws = wb.active\n",
    "\n",
    "# Loop over all columns\n",
    "for i, col in enumerate(df.columns, start=1):  # Excel columns are 1-indexed\n",
    "    # Hide if column name does NOT contain \"total\" and is not first two columns\n",
    "    if i > 2 and \"total\" not in col.lower():\n",
    "        ws.column_dimensions[ws.cell(row=1, column=i).column_letter].hidden = True\n",
    "\n",
    "# Save workbook\n",
    "wb.save(output_file)\n",
    "print(f\"Saved Excel with hidden columns to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
